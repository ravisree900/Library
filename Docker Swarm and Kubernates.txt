Container Orchestration
==============================
This is the process of handling docker containers running
on multiple linux servers in a distributed environment


Advantages
=================
1 Load Balancing
2 Scalling
3 Rolling update
4 High Availability and Disaster recovery(DR)

LoadBalancing
==================
Each container is capable of sustaining a specific user load
We can increase this capacity by running the same application
on multiple containers(replicas)

Scalling
==============
We should be able to increase or decrease the number of containers
on which our applications are running without the end user
exepriencing any downtime.

Rolling update
=======================
Application running in a live environment should be upgraded or
downgraded to a different version without the end user having any
downtime

Disaster Recovery
======================
In case of network failuers or server crashes still the container
orchestration tools maintain the desired count of containers
and thereby provide the same service to the end user

Popular container orchestration tools
===========================================
1 Docker Swarm
2 Kubernetes
3 OpenShift
4 Mesos


===========================================================================
Setup of Docker Swarm
============================
1 Create 3 AWS ubuntu instances
2 Name them as Manager,Worker1,Worker2
3 Install docker on all of them
4 Change the hostname
  vim /etc/hostname
  Delete the content and replace it with Manager or Worker1 or Worker2
5 Restart
  init 6
6 To initilise the docker swarm
  Connect to Manager AWS instance
  docker swarm init
  This command will create a docker swarm and it will also generate
  a tokenid
7 Copy and paste the token id in Worker1 and Worker2

===============================================================================
Ports used by docker swarm
======================================
TCP port 2376 for secure Docker client communication. This port is required for Docker Machine to work. Docker Machine is used to orchestrate Docker hosts.

TCP port 2377. This port is used for communication between the nodes of a Docker Swarm or cluster. It only needs to be opened on manager nodes.

TCP and UDP port 7946 for communication among nodes (container network discovery).
UDP port 4789 for overlay network traffic (container ingress networking).


=================================================================================
Load Balancing:
Each docker containers has a capability to sustain a specific
user load.To increase this capability we can increase the 
number of replicas(containers) on which a service can run

UseCase
------------
Create nginx with 5 replicas and check where these replicas are
running

1 Create nginx with 5 replicas
  docker service create --name webserver -p 8888:80 --replicas 5 nginx

2 To check the services running in swarm
  docker service ls

3 To check where these replicas are running
  docker service ps webserver

4 To access the ngonx from browser
  public_ip_of_manager/worker1/worker2:8888

5 To delete the service with all replicas
  docker service rm webserver
=========================================================================
UseCase
===========
Create mysql with 3 replicas and also pass the necessary environment
variables

1 docker service create --name db --replicas 3 
                    -e MYSQL_ROOT_PASSWORD=intelliqit  mysql:5

2 To check if 3 replicas of mysql are running
  docker service ps db

==========================================================================
Day 16
==========================================================================

Scalling
============
This is the process of increasing the number of replicas or decreasing
the replicas count based on requirement without the end user experiencing
any down time.

UseCase
============
Create tomcat with 4 replicas and scale it to 8 and scale it
down to 2 

1 Create tomcat with 4 replicas
  docker service create --name appserver -p 9090:8080 --replicas 4 tomcat

2 Check if 4 replicas are running
  docker service ps appserver

3 Increase the replicas count to 8
  docker service scale appserver=8

4 Check if 8 replicas are running
  docker service ps appserver

5 Decrese the replicas count to 2
  docker service scale appserver=2

6 Check if 2 replicas are running
  docker service ps appserver

=======================================================================
Rolling updates
======================
Services running in docker swarm should be updated from once
version to other without the end user downtime

UseCase
===========
Create redis:3 with 5 replicas and later update it to redis:4
also rollback to redis:3

1 Create redis:3 with 5 replicas
  docker service create --name myredis --replicas 5 redis:3

2 Check if all 5 replicas of redis:3 are running
  docker service ps myredis

3 Perfrom a rolling update from redis:3 to redis:4
  docker service update --image redis:4 myredis

4 Check redis:3 replcias are shut down and in tis palce redis:4 replicas are running
  docker service ps myredis

5 Roll back from redis:4 to redis:3
  docker service update --rollback myredis

6 Check if redis:4 replicas are shut down and in its place redis:3 is running
  docker service ps myredis


================================================================================
To remove a worker from swarm cluster
docker node update --availability drain Worker1

To make this worker rejoin the swarm
docker node update --availability active Worker1

To make worker2 leave the swarm
Connect to worker2 usig git bash
docker swarm leave

To make manager leave the swarm
docker swarm leave --force

To generate the tokenid for a machine to join swarm as worker
docker swarm join-token worker

To generate the tokenid for a machine to join swarm as manager
docker swarm join-token manager

To promote Worker1 as a manager
docker node promote Worker1

To demote "Worker1" back to a worker status
docker node demote Worker1



====================================================================
FailOver Scenarios of Workers
================================
Create httpd with 6 replicas and delete one replica running on the manager
Check if all 6 replicas are still running

Drain Worker1 from the docker swarm and check if all 6 replicas are running
on Manager and Worker2,make Worker1 rejoin the swarm

Make Worker2 leave the swarm and check if all the 6 replicas are
running on Manager and Worker1

1 Create httpd with 6 replicas
  docker service create  --name webserver -p 9090:80 --replicas 6 httpd

2 Check the replicas running on Manager
  docker service ps webserver | grep Manager

3 Check the container id
  docker container ls

4 Delete a replica
  docker rm -f container_id_from_step3

5 Check if all 6 replicas are running
  docker service ps webserver

6 Drain Worker1 from the swarm
  docker node update --availability drain Worker1

7 Check if all 6 replicas are still running on Manager and Worker2
  docker service ps webserver

8 Make Worker1 rejoin the swarm
  docker node update --availability active Worker1

9 Make Worker2 leave the swarm
  Connect to Worker2 using git bash
  docker swarm leave
  Connect to Manager
  
10 Check if all 6 replicas are still running
   docker service ps webserver


==========================================================================
Day 17
=========================================================================
 ======================================================================
FailOver Scenarios of Managers
====================================
If a worker instance crashses all the replicas running on that
worker will be moved to the Manager or the other workers.
If the Manager itself crashes the swarm becomes headless 
ie we cannot perfrom container orchestration activites in this
swamr cluster

To avoid this we should maintain multiple managers
Manager nodes have the status as Leader or Reachable

If one manager node goes down other manager becomes the Leader
Quorum is resonsible for doing this activity and if uses a RAFT
algorithm for handling the failovers of managers.Quorum also 
is responsible for mainting the min number of manager

Min count of manager required for docker swarm should be always
more than half of the total count of Managers

Total Manager Count  -    Min Manager Required
      1              -           1
      2              -           2
      3              -           2
      4              -           3
      5              -           3
      6              -           4
      7              -           4




===========================================================================
==================================================================================
Overlay Networking
=========================
This is the deafult network used by swarm
and this network perfroms network load balancin
ie even if a service is running on a specicfic  worker we can
access if from orther slave


=============================================================================
UseCase
===========
Create 2 overlay networks intelliqit1 and intelliqit2
Create httpd with 5 replacs on intelliqit1 network
Create tomcat with 5 replicas on default overlay "ingres" network
and later perform rolling network update to intelliqit2 network

1 Create 2 overlay networks
  docker network create  --driver overlay intelliqit1
  docker network create  --driver overlay intelliqit2

2 Check if 2 overlay networks are created
  docker network ls

3 Create httpd with 5 replcias on inteliiqit1 network
  docker service create  --name webserver -p 8888:80 --replicas 5 
                                           --network intelliqit1 httpd

4 To check if httpd is running on intelliqit1 network
  docker service inspect webserver
  This command will generate the output in JSON format
  To see the above output in normal text fromat
  docker service inspect webserver --pretty

5 Create tomcat with 5 replicas on the deafult ingres network
  docker service create --name appserver -p 9999:8080 --replicas 5 tomcat

6 Perform a rolling network update from ingres to intelliqit2 network
  docker service update --network-add intelliqit2 appserver

7 Check if tomcat is now running on intelliqit2 network
  docker service inspect appserver --pretty

Note: To remove from intelliqit2 network
      docker service update --network-rm intelliqit2 appserver

===============================================================================
Day 18
=============================================================================   
==================================================================================
Overlay Networking
=========================
This is the deafult network used by swarm
and this network perfrom network load balancin
ie even if a service is running on a specicfic  worker we can
access if from orther slave

UseCase
=============
Start nginx with 2 repliacs and check if we can acces it from 
browser from manager and all workers

1 Create nginx
  docker service create  --name webserver -p 8888:80 --replicas 2 nginx

2 Check where these 2 replcas are running
  docker service ps webserver
  These repliacs will be running on only 2 nodes and we will have a third
  node where it it not running

3 Check if we can access nginx from the third node where it is not present
  public_ip_of_thirdnode:8888

=============================================================================
Docker Stack
=====================
docker compose + docker swarm = docker stack
docker compose + kubernetes = kompose

Docker compose when implemented at the level of docker swarm
it is called docker stack.Using docker stack we can create an orchestreta
a micro services architecture at the level of production servers

1 To create a stack from a compose file
  docker stack deploy -c compose_filename stack_name

2 To see the list of stacks created
  docker stack ls

3 To see on which nodes the stack services are running
  docker stack ps stack_name

4 To delete a stack
  docker stack rm stack_name

=====================================================================
UseCase
================
Create a docker stack file to start 3 replicas of wordpress
and one replica of mysql

vim stack1.yml
---
version: '3.8'

services:
 db:
  image: "mysql:5"
  environment:
   MYSQL_ROOT_PASSWORD: intelliqit

 wordpress:
  image: wordpress
  ports:
   - "8989:80"
  deploy:
   replicas: 3

To start the stack file
docker stack deploy -c stack1.yml mywordpress

To see the services running
docker service ls

To check where the serives are running
docker stack ps mywordpress

To delete the stack
docker stack rm mywordpress


================================================================================
UseCase
==============
Create a stack file to setup CI-cd architecture where a jenkins
container is linked with tomcats for qa and prod environments
The jenkins contianers should run only on Manager
the qaserver tomcat should run only on Worker1 and prodserver
tomcat should run only on worker2

vim stack2.yml
---
version: '3.8'

services:
 myjenkins:
  image: jenkins/jenkins
  ports:
   - 5050:8080
  deploy:
   replicas: 2
   placement:
    constraints:
     - node.hostname == Manager

 qaserver:
  image: tomcat
  ports:
   - 6060:8080
  deploy:
   replicas: 3
   placement:
    constraints:
     - node.hostname == Worker1

 prodserver:
  image: tomcat
  ports:
   - 7070:8080
  deploy:
   replicas: 4
   placement:
    constraints:
     - node.hostname == Worker2
...

To start the services 
docker deploy -c stack2.yml ci-cd

To check the replicas 
docker stack ps ci-cd
=====


============================================================
Day 19
=============================================================
UseCase
Create a stack file to setup the selenium hub and nodes architecture
but also specify a upper limit on the h/w

vim stack3.yml
---
version: '3.8'

services:
 hub:
  image: selenium/hub
  ports:
   - 4444:4444
  deploy:
   replicas: 2
   resources:
    limits:
     cpus: "0.1"
     memory: "300M"

 chrome:
  image: selenium/node-chrome-debug
  ports:
   - 5901:5900
  deploy:
   replicas: 3
   resources:
    limits:
     cpus: "0.01"
     memory: "100M"

 firefox:
  image: selenium/node-firefox-debug
  ports:
   - 5902:5900
  deploy:
   replicas: 3
   resources:
    limits:
     cpus: "0.01"
     memory: "100M"
===================================================================
Docker secrets
===========================
This is a feature of docker swarm using which we can pass secret data
to the services running in swarm cluster
These secrets are created on the host machine and they will be
availbale from all the replicas in the swarm cluster

1 Create a dcoker secret
  echo " Hello Intelliqit" | docker secret create mysecret -

2 Create a redis db with 5 replace and mount the secret
  docker service create --name myredis --replicas 5 --secret mysecret redis

3 Capture one of the replica contianer id
  docker container ls

4 Check if the secret data is available
  docker exec -it container_id cat /run/secrets/mysecret


==============================================================================
Create 3 secrets for postgres user,password and db
and pass them to the stack file

1 Create secrets
  echo "intelliqit" | docker secret create pg_password -
  echo "myuser" | docker secret create pg_user -
  echo "mydb" | docker secret create pg_db -

2 Check if the secrets are created
  docker secret ls

3 Create the docker stack file to work on these secrets
  vim stack6.yml
---
version: '3.1'
services:
  db:
    image: postgres
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/pg_password
      POSTGRES_USER_FILE: /run/secrets/pg_user
      POSTGRES_DB_FILE: /run/secrets/pg_db
    secrets:
     - pg_password
     - pg_user
     - pg_db

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
    deploy:
     replicas: 2

secrets:
    pg_password:
     external: true
    pg_user:
     external: true
    pg_db:
     external: true

...
=========================================================================
Working on docker registry
==========================
This is the location where the docker images are saved
This is of 2 types
1 Public registry
2 Private regsitry

UseCase
Create a customised ubuntu image and upload into the public registry

1 Signup into hub.docker.com

2 Create a customsied ubuntu image
  a) Create a centos container and install git init
     docker run --name u1 -it ubuntu
    apt-get update
    apt-get  install -y  git
    exit

  b) Save this container as an image
     docker commit u1 intelliqit/ubuntu_may_25

3 Login into dockerhub
  docker login
  Enter username and password of dockerhub

4  Push the customised image
   docker push intelliqit/ubuntu_may_25


========================================================================
Private Registry
=====================
This can be created using a docker image called as "registry"
We can start this as a container and it will allow us to 
push images into the registry

1 Create registry as a container
  docker run --name lr -d -p 5000:5000 registry

2 Download an alpine image
  docker pull alipne

3 Tag the alpine with the local registry
  docker tag alpine localhost:5000/alpine

4 Push the image to local registry
  docker push localhost:5000/alpine

============================================================================
Day 20
=============================================================================
Kubernetes 
===================

Menions: This is an individual node used in kubernetes
Combination of these minions is called as Kubernetes cluster

Master is the main machine which triggers the container orchestraion
It distributes the work load to the Slaves

Slaves are the nodes that accept the work load from the master
and handle activites load balancing,autoscalling,high availability etc


==============================================
Kubernetes unmanaged setup on Centos
=========================================
Install, start and enable docker service

yum install -y -q yum-utils device-mapper-persistent-data lvm2 > /dev/null 2>&1
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo > /dev/null 2>&1
yum install -y -q docker-ce >/dev/null 2>&1


systemctl start docker
systemctl enable docker

=====================================================================================
Disable SELINUX

setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux

============================================================================================
Disable SWAP

sed -i '/swap/d' /etc/fstab
swapoff -a

===========================================================================================
Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

============================================================================================
Add Kubernetes to yum repository

cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

======================================================================================
Install Kubernetes
yum install -y kubeadm kubelet kubectl

==================================================================================
Enable and start Kubernetes service

systemctl start kubelet
systemctl enable kubelet
=====================================================================================
Repeat the above steps on Master and slaves
=======================================================================================

On Master=============
===========
Initilise the Kubernetes cluster
-----------------------------------------

kubeadm init --apiserver-advertise-address=ip_of_master --pod-network-cidr=192.168.0.0/16

=========================================================================================

To be able to use kubectl command to connect and interact with the cluster, 
the user needs kube config file.

mkdir /home/centos/.kube
cp /etc/kubernetes/admin.conf /home/centos/.kube/config
chown -R centos:centos /home/centos/.kube

========================================================================================
Deploy calico network
kubectl create -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml

========================================================================================
For slaves to join the cluster
kubeadm token create --print-join-command

======================================================================================
Check the pods of kube-system  are running

kubectl get pods -n kube-system

============================================================================
Day 21
=========================================================================
Kubernetes
======================

Menions: This is an individual node used in kubernetes
Combination of these minions is called as Kubernetes cluster

Master is the main machine which triggers the container orchestraion
It distributes the work load to the Slaves

Slaves are the nodes that accept the work load from the master
and handle activites load balancing,autoscalling,high availability etc



Kubernetes uses various of types of Object

1 Pod: This is a layer of abstraction on top of a container.This is the samallest
  object that kubernetes can work on.In the Pod we have a container.
  The advantage of using a Pod is that kubectl commands will work on the Pod and the 
  Pod communicates these instructions to the container.In this way we can use the
  same  kubectl irresepective of which technology containers are in the Pod.



2 Service: This is used for port mapping and network load balancing

3 NameSpace: This is used for creating partitions in the cluster.Pods running
 in a namespace cannot communicate with other pods running in other namespace

4 Secrets: This is used for passing encrypted data to the Pods

5 ReplicationController: This is used for managing multiple replicas of PODs
and also perfroming saclling 

6 ReplicaSet: This is similar to replicationcontroller but it is more advanced
where features like selector can be implemented

7 Deployment: This used for perfroming all activites that a Replicaset can do
  it can also handle rolling update

8 Volume: Used to preserve the data even when the pods are deleted

9 Statefulsets: These are used to handle stateful application like data bases
  where consistency in read write operations has to be maintained.

10 Ingress: This object is used for mapping ip with domain name 

Kubernetes Architecture
=============================
Master Componentes
=======================
Container runtime: This can be docker or anyother container technology

apiServer: Users interact with the apiServer using some clinet like ui,command line tool like kubelet.It is the apiServer which is the gateway to the cluster
It works as a gatekeeper  for authentication and it validates if a specific
user is having permissions to execute a specific command.Example if we want to
deploy a pod or a deployment first apiServers validates if the user is authorised to perform that action and if so it passes to the next process
ie the "Scheduler"

Scheduler: This process accepts the instructions from apiServer after validation
and starts an application on a sepcific node or set of nodes.It estimates
how much amount of h/w is required for an application and then checks which
slave have the necessary h/w resources and instructs the kubelet to deploy
the application

kubelet: This is the actual process that takes the orders from scheduler and
deploy an application on a slave.This kubelet is present on both master and slave

controller manager: This check if the desired state of the cluster is always
maintained.If a pod dies it recreates that pod to maintain the desired state

etcd: Here the cluster state is maintained in key value pairs.
It maintains info about the slaves and the h/w resources available on
the slaves and also the pods running on the slaves
The scheduler and the control manager read the info from this etcd
and schedule the pods and maintain the desired state

===========================================================================
Worker components
=======================
containerrun time: Docker or some other container technology

kubelet: This process interacts with container run time and the node 
and it start a pod with a container in it

kubeproxy: This will take the request from services to pod
It has the intellegence to forward a request to
a near by pod.Eg If an application pod wants to communicate with a db pod
then kubeproxy will take that request to the nearby pod 

=================================================================================
=============================================================================
Setup of ManagedKubernetes
===============================
Free
===========
1 http://katakoda.com
(or)
2 http://playwithk8s.com

Paid
==============
1 Signup for a Google cloud account
2 Click on Menu icon on top right corner--->Click on Kubernetes Engine-->Clusters
3 Click on Create cluster--->Click on Create

================================================================================
Day 22
================================================================================
=========================================================================
UseCase
===========
Create nginx as a pod and name it webserver
kubectl run --image nginx webserver

To see the list of pods running
kubectl get pods

To see more info about the pods like their ip and slave where they are running
kubectl  get pods -o wide

To delete the pod
kubectl delete pods webserver

============================================================================
UseCase
========= 
Create mysql pod and name it mydb and go into its interactive terminal and create few tables

kubectl run --image mysql:5 mydb --env MYSQL_ROOT_PASSWORD=intelliqit

To check the pods
kubectl get pods

To go into the interactive terminal
kubectl exec -it mydb -- bash

To login into the db
mysql -u root -p
Password: intellqiit

Create tables here

=========================================================================
Kuberentes Defintion files
==============================
Objects in Kubernetes cluster are deployed using these
defintion files 
They are created using yml and they generally these 4 top level 
fields.

apiVersion:
kind:
metadata:
spec:

apiVersion : This specifies the code library that has to be imported
to create a particualr kind of Kubernetes object

kind: Here we specify the type kubernetes object that we want to 
create(Pod,ReplicaSet,Deployment,Service etc)

metadata: Here we can give additional info about the Pod like
the name of the Pod,some labels etc

spec: This is where exact info about the object that is created is
specified like containers info port mapping,no of replicas etc

================================================================
kind                     apiVersions
===================================================
Pod			 v1
Service			 v1
Secret			 v1
Namespace		 v1
ReplicationController    v1
Volume		         v1
ReplicaSet  		 apps/v1
Deployment	         apps/v1
StatefuleSet             apps/v1


==================================================================
Create a pod defintion file to start nginx pod with a name webserver

1 vim pod-defintion1.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: nginx-pod
 labels:
  type: proxy
  author: intelliqit
spec:
 containers:
  - name: webserver
    image: nginx
   
...

2 Create pod from the above file
  kubectl apply -f pod-defintion1.yml

3 To check the list of pods
  kubectl get pods

4 To delete the pods
  kubectl delete -f pod-defintion1.yml

========================================================================
UseCase
================
Create a postgres-pod and give the labels as author=intelliqit
and type=db,also pass the necessay environment variables

1 vim pod-definition2.yml
apiVersion: v1
kind: Pod
metadata:
 name: postgres-pod
 labels:
  author: intelliqit
  type: db
spec:
 containers:
  - name: mydb
    image: postgres
    env:
     - name: POSTGRES_PASSWORD
       value: intelliqit
     - name: POSTGRES_USER
       value: myuser
     - name: POSTGRES_DB
       value: mydb
...

To create pods from the above file
kubectl apply -f pod-defintion2.yml


====================================================================
UseCase
============
Create a jenkins-pod and also perfrom necessary port mapping

vim pod-definition2.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: jenkins-pod
 labels:
  type: ci-cd
  author: intelliqit
spec:
 containers:
  - name: jenkins
    image: jenkins/jenkins
    ports:
     - containerPort: 8080
       hostPort: 8080
...

To create the pods from the above file
kubectl apply -f pod-defintion3.yml

To check if the jnekins pod is running
kubectl get pods -o wide

To accesss jenkins from browser
kubectl get nodes -o wide
Capture the external ip of the node where jenkins pod is running
in browser
externalip:8080

============================================================================
Day 23
=============================================================================
========================================================================
ReplicationController
=======================
This is a high level Kubernets object that can be used for handling 
multiple replicas of a Pod.Here we can perfrom Load Balancing
and Scalling

ReplicationController uses keys like "replicas,template" etc in the "spec" section
In the template section we can give metadata related to the pod and also use
another spec section where we can give containers information

Create a replication controller for creating 3 replicas of httpd
vim repilication-controller.yml
---
apiVersion: v1
kind: ReplicationController
metadata:
 name: httpd-rc
 labels:
  author: intelliqit
spec:
 replicas: 3
 template:
  metadata:
   name: httpd-pod
   labels:
    author: intelliqit
  spec:
   containers:
    - name: myhttpd
      image: httpd
      ports:
       - containerPort: 80
         hostPort: 8080
...

To create the httpd replicas from the above file
kubectl create -f replication-controller.yml

To check if 3 pods are running an on whcih slaves they are running
kubectl get pods -o wide

To delete the replicas
kubectl delete -f replication-controller.yml



ReplicaSet
===================
This is also similar to ReplicationController but it is more
advanced and it can also handle load balancing and scalling
It has an additional field in spec section called as "selector"
This selector uses a child element "matchLabels" where the
it will search for Pod based on a specific label name and try to add
them to the cluster

Create a replicaset file to start 4 tomcat replicas  and then perform scalling
vim replica-set.yml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: tomcat-rs
 labels:
  type: webserver
  author: intelliqit
spec:
 replicas: 4
 selector:
  matchLabels:
   type: webserver
   
 template:
  metadata:
   name: tomcat-pod
   labels:
    type: webserver
  spec:
   containers:
    - name: mywebserver
      image: tomcat
      ports:
       - containerPort: 8080
         hostPort: 9090

To create the pods from the above file
kubectl create -f replica-set.yml

Scalling can be done in 2 ways
a) Update the file and later scale it

b) Scale from the coomand prompt withbout updating the defintion file

a) Update the file and later scale it
  Open the replicas-set.yml file and increase the replicas count from 4 to 6
  kubectl replace -f replicas-set.yml
  Check if 6 pods of tomcat are running
  kubectl get pods

b) Scale from the coomand prompt withbout updating the defintion file
   kubectl scale --replicas=2 -f replica-set.yml


================================================================
Deployment
================

This is also a high level Kubernetes object which can be used for
scalling and load balancing and it can also perfrom rolling update

Create a deployment file to run nginx:1.7.9 with 3 replicas


vim deployment1.yml
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: nginx-deployment
 labels:
  author: intelliqit
  type: proxyserver
spec:
 replicas: 3
 selector:
  matchLabels:
   type: proxyserver
 template:
  metadata:
   name: nginx-pod
   labels:
    type: proxyserver
  spec:
   containers:
    - name: nginx
      image: nginx:1.7.9
      ports:
       - containerPort: 80
         hostPort: 8888
 
To create the deployment from the above file
kubectl create -f deployment.yml

To check if the deployment is running
kubectl get deployment

To see if all 3 pod of nginx are running
kubectl get pod

Check the version of nginx
kubectl describe pods nginx-deployment | less

==========================================================================
Namespace in kubernetes
==========================
Namespaces are used to create partitions in the Kubernetes cluster
Pods runnign in different namespaces cannot communicate with
each other

To create Namespaces
===========
vim namespace.yml
---
apiVersion: v1
  kind: Namespace
  metadata:
    name: test-ns
...

kubectl apply -f namespace.yaml 

To see the list of namespace
================================
kubectl get namespace

Create a pod on that namespace
===================================
vim pod-definition4.yml

---
apiVersion: v1
kind: Pod
metadata:
 name: jdk-pod
 namespace: test-ns
 labels:
  author: intelliqit
spec:
 containers:
  - name: java
    image: openjdk:12
...

To see list of pods in a namespace
======================================
kubectl get pods -n test-ns

To delete a namespace
===========================
kubectl delete namespace test-ns

==========================================================================
Day 24
==========================================================================
Kompose
================
This is used to implement docker compose to create a multi
container architecture in Kubernetes

Implementing docker compose can be done using Kompose
docker compose + docker swarm = docker stack
docker compose + Kubernetes = Kompose

Setup
===========
1 Download Kompose
  curl -L https://github.com/kubernetes/kompose/releases/download/v1.18.0/kompose-linux-amd64 -o kompose

2 Give execute permissions
  chmod +x kompose

3 Move it to PATH
  sudo mv ./kompose /usr/local/bin/kompose

4 To check if the installion is successfull
  kompose version

Digital Ocean URL
========================
https://www.digitalocean.com/community/tutorials/how-to-migrate-a-docker-compose-workflow-to-kubernetes

Create a docker compose file
vim docker-compose.yml
---
version: '3'

services:
 mydb:
  image: mysql:5
  environment:
   MYSQL_ROOT_PASSWORD: intelliq

 wordpress:
  image: wordpress
  ports:
   - 8080:80
  deploy:
   replicas: 3
...

To setup the above architecture in Kubernetes
kompose up

To create kubernetes definition files
kompose convert

To delete the above create architecture
kompose down

Note: Practice Kompsoe on katokoda.com
======================================================================
---
apiVersion: v1
kind: Pod
metadata:
 name: redis-pod
 labels:
  author: intelliqit
spec:
 containers:
  - name: redis
    image: redis
    volumeMounts:
     - name: redis-volume
       mountPath: /data/redis
 volumes:
  - name: redis-volume
    emptyDir: {}

Create a pod from the above file
kubectl create -f volumes.yml

To check if the volume is mounted
kubectl exec -it redis-pod -- bash

Go to the redis folder and create some files
cd redis
cat > file
Store some data in this file

To kill the redis pod install procps
apt-get update
apt-get install -y procps

Identify the process id of redis
ps aux
kill 1

Check if the redis-pod is recreated
kubectl get pods
We will see the restart count changes for this pod

If we go into this pods interactive terminal
kubectl exec -it redis-pod -- bash

We will see the data but not the s/w's (procps) we installed
cd redis
ls

ps  This will not work

============================================================================
Day 25
============================================================================
==============================================================
Service Object
=====================

This is used for network load balancing and port mapping
It uses 3 ports
1 target port:  Pod or container port
2 port:   Service port
3 hostPort:  Host machines port to make it accessable from external network

Service objects are classified into 3 types
1 clusterIP: This is the default type of service object used in
  Kubernetes and it is used when we want the Pods in the cluster to
  communicate with each other and not with extrnal networks

2 nodePort: This is used if we want to access the pods from an extrnal
  network and it also performs network load balancing ie even if a pod
  is running on a specific salve we can access it from other slave in
  the cluster

3 LoadBalancer: This is similar to Nodeport and it is used for external 
  connectivity of a Pod and also network load balancing and it also assigns
  a public ip for all the slave combined together


=============================================================================
Use Case
=================
Create a service defintion file for port mapping an nginx pod

vim pod-defintion1.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: nginx-pod
 labels:
  author: intellqit
  type: proxy
spec:
 containers:
  - name: appserver
    image: nginx
=========================================================
vim service1.yml
---
apiVersion: v1
kind: Service
metadata:
 name: nginx-service
spec:
 type: NodePort
 ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
 selector:
  author: intellqit
  type: proxy

Create pods from the above pod definition file
kubectl create -f pod-definition1.yml
Create the service from the above service definition file
kubectl create -f service.yml
Now nginx can be accesed from any of the slave
kubectl get nodes -o wide
Take the external ip of any of the nodes:30008


UseCase
==================
Create a pod defintion file to start a ghost pod and also create a
service object of the type LoadBalancer

vim pod-defintion7.yml
---
apiVersion: v1
kind: Pod
metadata:
 name: ghost-pod
 labels:
  author: intelliqit
  type: CMS
spec:
 containers:
  - name: ghost
    image: ghost
...

vim service2.yml
---
apiVersion: v1
kind: Service
metadata:
 name: ghsot-service
 labels:
  author: intelliqit
spec:
 type: LoadBalancer
 ports:
  - targetPort: 2368
    port: 2368
 selector:
  type: CMS
  author: intelliqit

=====================================================================
UseCase
================
Create a pod-definiton file for httpd pod and create a service object
of type cluster ip for it

vim pod-definition8.yml
---
apiVersion: v1
kind: Service
metadata:
 name: ghsot-service
 labels:
  author: intelliqit
spec:
 type: LoadBalancer
 ports:
  - targetPort: 2368
    port: 2368
 selector:
  type: CMS
  author: intelliqit
...

vim service3.yml
---
apiVersion: v1
kind: Service
metadata:
 name: httod-service
 labels:
  author: intelliqit
spec:
 ports:
  - targetPort: 80
    port: 80
 selector:
  author: intelliqit
  type: webserver
...

=========================================================================
UseCase
==============
Create a deployment file of for tomcat and also create a servcie file
of type node port

vim deployment3.yml
---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: tomcat-deployment
 labels:
  type: appserver
spec:
 replicas: 2
 selector:
  matchLabels:
   type: appserver
 template:
  metadata:
   name: tomcat-pod
   labels:
    type: appserver
  spec:
   containers:
    - name: tomcat
      image: tomee
...

vim service4.yml
---
apiVersion: v1
kind: Service
metadata:
 name: tomcat-service
 labels:
  author: intelliqit
spec:
 type: NodePort
 ports:
  - targetPort: 8080
    port: 8080
 selector:
  type: appserver
...

==========================================================================


===========================================================================
Day 26
===========================================================================
=======================================================================
Kubernetes Project
========================
This is a python based application which is used for accepting a vote
(voting app).This application accepts the vote and passes it to a
temporary db created using redis.From here the data is passed to a
worker application created using .net which anlysises the data and
stores them permananatly in a database created using postgres
From here the results can be seen on an application that is created 
using nodejs and this is called as resulta-app

To do this we will create 5 pod definition files
and 4 service files,2 services of type cluster ip for redis and postgres 
databases 2 services of type loadbalancer for python voting app and 
nodejs result app

Pod Definition Files
================================
vim voting-app-pod.yml

---
apiVersion: v1
kind: Pod
metadata:
  name: voting-app-pod
  labels:
    name: voting-app-pod
    author: intelliqit
spec:
  containers:
    - name: voting-app
      image: dockersamples/examplevotingapp_vote
      ports:
        - containerPort: 80
...


vim result-app-pod.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: result-app-pod
  labels:
    name: result-app-pod
    author: intelliqit
spec:
  containers:
    - name: result-app
      image: dockersamples/examplevotingapp_result
      ports:
        - containerPort: 80
...


vim worker-app-pod.yml

---
apiVersion: v1
kind: Pod
metadata:
  name: worker-app-pod
  labels:
    name: worker-app-pod
    author: intelliqit
spec:
  containers:
    - name: worker-app
      image: dockersamples/examplevotingapp_worker
...


vim redis-pod.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: redis-pod
  labels:
    name: redis-pod
    author: intelliqit
spec:
  containers:
   - name: redis
     image: redis
     ports:
       - containerPort: 6379
...

vim postgres-pod.yml

---
apiVersion: v1
kind: Pod
metadata:
  name: postgres-pod
  labels:
    name: postgres-pod
    author: intelliqit
spec:
  containers:
    - name: postgres
      image: postgres
      ports:
        - containerPort: 5432
...


============================================================================
Service Defintion file
===============================
vim redis-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  labels:
    name: redis-service
    author: intelliqit
spec:
  ports:
    - port: 6379
      targetPort: 6379
  selector:
    name: redis-pod
    app: demo-voting-app
...

vim pod-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  labels:
    name: postgres-service
    author: intelliqit
spec:
  ports:
    - port: 5432
      targetPort: 5432
  selector:
    name: postgres-pod
    app: demo-voting-app
...

Note: Since "type" is not specified in the "spec" section they  will
be created as clusterIP




vim voting-app-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: voting-app-service
  labels:
    name: voting-app-service
    author: intelliqit
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: voting-app-pod
    app: demo-voting-app
...

vim result-app-service.yml
---
apiVersion: v1
kind: Service
metadata:
  name: result-app-service
  labels:
    name: result-app-service
    author: intelliqit
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: result-app-pod
    app: demo-voting-app
...


The above 2 service objects are created as LoadBalancer type ie
they can perfrom network load balancing where we can access the pod
from any slave and also a single public ip will be assigned for all
the salves

========================================================================
Day 30
========================================================================
To deploy the above project using docker stack
vim voting-app.yml
---
version: '3'

services:
 voting-app:
  image: dockersamples/examplevotingapp_vote
  ports:
   - 6060:80

 redis:
  image: redis
  ports:
   - 6379:6379

 worker-app:
  image: dockersamples/examplevotingapp_worker

 postgres:
  image: postgres
  environment:
   POSTGRES_PASSWORD: intelliqit
  ports:
   - 5432:5432

 result-app:
  image: dockersamples/examplevotingapp_result
  ports:
   - 7070:80


To deploy the above services
docker stack deploy -c voting-app.yml my-voting-app


To see the list of nodes where the stack services are running
docker stack ps my-voting-app

============================================================================
To deploy the above file in kubernetes using kompose
==============================================================================
Kompose
================
This is used to implement docker compose to create a multi
container architecture in Kubernetes

Implementing docker compose can be done using Kompose
docker compose + docker swarm = docker stack
docker compose + Kubernetes = Kompose

Setup
===========
1 Download Kompose
  curl -L https://github.com/kubernetes/kompose/releases/download/v1.18.0/kompose-linux-amd64 -o kompose

2 Give execute permissions
  chmod +x kompose

3 Move it to PATH
  sudo mv ./kompose /usr/local/bin/kompose

4 To check if the installion is successfull
  kompose version

Digital Ocean URL
========================
https://www.digitalocean.com/community/tutorials/how-to-migrate-a-docker-compose-workflow-to-kubernetes

=================================================================================
Day 27
=================================================================================
Stateful set 
======================
---
apiVersion: v1
kind: Service
metadata:
 name: nginx
 labels:
  app: nginx
spec:
 ports:
  - port: 80
    name: web
 clusterIP: None
 selector:
  app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
 name: web
spec:
 serviceName: "nginx"
 replicas: 3
 selector:
  matchLabels:
   app: nginx
 template:
  metadata:
   labels:
    app: nginx
  spec:
   containers:
    - name: nginx
      image: nginx
      ports:
       - containerPort: 80
         name: web
      volumeMounts:
       - name: www
         mountPath: /usr/share/nginx/html
 volumeClaimTemplates:
  - metadata:
     name: www
    spec:
     accessModes: [ "ReadWriteOnce" ]
     resources:
      requests:
       storage: 1Gi


==============================================================================
Node affinity
=================================
kubectl get nodes --show-labels

kubectl label nodes <your-node-name> disktype=ssd

kubectl label nodes gke-cluster-1-default-pool-3cde7c4a-hl74 disktype=ssd
=====================================================================
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd            
  containers:
  - name: nginx
    image: nginx
    
================================================================
Taints and toleration
========================
Taints and Tolerations
Node affinity, is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods.

Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.

Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.

To create a taint for a node
kubectl taint nodes node1 node=intelliqit:NoSchedule

To delete the tain
kubectl taint nodes node1 node=intelliqit:NoSchedule-

Pod defintion file to use the above taint
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    author: intelliqit
spec:
  containers:
  - name: mygninx
    image: nginx
  tolerations:
  - key: "node"
    operator: "Equal"
    value: "intelliqit"
    effect: "NoSchedule"


===================================================================
Helm and Kubernetes
Helm is a package manager. Package managers automate the process of installing, configuring, upgrading, and removing computer programs


Helm has two elements, a client (Helm) and a server (Tiller). The server element runs inside a Kubernetes cluster and manages the installation of charts.	